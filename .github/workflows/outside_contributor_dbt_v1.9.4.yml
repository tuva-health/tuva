name: outside_contributor_dbt_v1.9.4

on:
  workflow_dispatch:
   inputs:
      prNumber:
        description: 'Pull Request Number'
        required: true

concurrency:
  # Ensure only one instance of this workflow runs across all PRs to prevent database conflicts
  group: ci-testing
  # Don't cancel in-progress runs automatically
  cancel-in-progress: false

env:
  PYTHON_VERSION: '3.11'
  LINT_SCRIPT_PATH: ./lint_tuva_project.sh
#######  DuckDB/MotherDuck
  DBT_MOTHERDUCK_TOKEN: ${{ secrets.DBT_MOTHERDUCK_TOKEN }}
#######  BigQuery
  TUVA_BIGQUERY_TOKEN: ${{ secrets.TUVA_BIGQUERY_TOKEN }}
  TUVA_BIGQUERY_PROJECT: ${{ secrets.TUVA_BIGQUERY_PROJECT }}
#######  Databricks
  DBT_DATABRICKS_HOST: ${{ secrets.DBT_DATABRICKS_HOST }}
  DBT_DATABRICKS_HTTP_PATH: ${{ secrets.DBT_DATABRICKS_HTTP_PATH }}
  DBT_DATABRICKS_TOKEN: ${{ secrets.DBT_DATABRICKS_TOKEN }}
  DBT_DATABRICKS_CATALOG: ${{ secrets.DBT_DATABRICKS_CATALOG }}
  DBT_DATABRICKS_SCHEMA: ${{ secrets.DBT_DATABRICKS_SCHEMA }}
#######  Fabric
  DBT_FABRIC_CI_HOST: ${{ secrets.DBT_FABRIC_CI_HOST }}
  DBT_FABRIC_SERVICE_PRINCIPAL_ID: ${{ secrets.DBT_FABRIC_SERVICE_PRINCIPAL_ID }}
  DBT_FABRIC_SERVICE_PRINCIPAL_SECRET: ${{ secrets.DBT_FABRIC_SERVICE_PRINCIPAL_SECRET }}
  DBT_FABRIC_TENANT_ID: ${{ secrets.DBT_FABRIC_TENANT_ID }}
  DBT_FABRIC_CI_DATABASE: ${{ secrets.DBT_FABRIC_CI_DATABASE }}
  DBT_FABRIC_CI_SCHEMA: ${{ secrets.DBT_FABRIC_CI_SCHEMA }}
#######  Redshift
  DBT_REDSHIFT_SERVERLESS_HOST: ${{ secrets.REDSHIFT_SERVERLESS_TEST_HOST }}
  DBT_REDSHIFT_SERVERLESS_USER: ${{ secrets.REDSHIFT_SERVERLESS_TEST_USERNAME }}
  DBT_REDSHIFT_SERVERLESS_PASSWORD: ${{ secrets.REDSHIFT_SERVERLESS_TEST_PASSWORD }}
  DBT_REDSHIFT_SERVERLESS_DATABASE: ${{ secrets.REDSHIFT_SERVERLESS_TEST_DATABASE }}
#######  Snowflake
  DBT_TUVA_SNOWFLAKE_ACCOUNT: ${{ secrets.DBT_TUVA_SNOWFLAKE_ACCOUNT }}
  DBT_TUVA_CI_DATABASE: ${{ secrets.DBT_TUVA_CI_DATABASE }}
  DBT_SNOWFLAKE_CI_PASSWORD: ${{ secrets.DBT_SNOWFLAKE_CI_PASSWORD }}
  DBT_SNOWFLAKE_CI_ROLE: ${{ secrets.DBT_SNOWFLAKE_CI_ROLE }}
  DBT_SNOWFLAKE_CI_SCHEMA: ${{ secrets.DBT_SNOWFLAKE_CI_SCHEMA }}
  DBT_SNOWFLAKE_CI_USER: ${{ secrets.DBT_SNOWFLAKE_CI_USER }}
  DBT_SNOWFLAKE_CI_WAREHOUSE: ${{ secrets.DBT_SNOWFLAKE_CI_WAREHOUSE }}

jobs:
  # ============================================================================
  # Linting Job (Runs First)
  # ============================================================================
  sqlfluff-lint:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          # Install dbt adapter needed for compile/lint (using Snowflake here)
          # Also install sqlfluff
          pip install dbt-core==1.9.4 dbt-snowflake sqlfluff==3.3.1 sqlfluff-templater-dbt

      # --- Setup dbt Profile for Linting ---
      - name: Create dbt profiles directory
        run: mkdir -p ~/.dbt

      - name: Create dbt profiles.yml for Snowflake (for linting context)
        run: |
          echo "default:
            outputs:
              dev:
                account: \"{{ env_var('DBT_TUVA_SNOWFLAKE_ACCOUNT') }}\"
                database: dev_ci_testing
                password: \"{{ env_var('DBT_SNOWFLAKE_CI_PASSWORD') }}\"
                role: \"{{ env_var('DBT_SNOWFLAKE_CI_ROLE') }}\"
                schema: \"{{ env_var('DBT_SNOWFLAKE_CI_SCHEMA') }}\"
                threads: 8
                type: snowflake
                user: \"{{ env_var('DBT_SNOWFLAKE_CI_USER') }}\"
                warehouse: \"{{ env_var('DBT_SNOWFLAKE_CI_WAREHOUSE') }}\"
            target: dev" > ~/.dbt/profiles.yml

      - name: Check if lint script exists
        run: |
          if [ ! -f "${{ env.LINT_SCRIPT_PATH }}" ]; then
            echo "Error: Lint script not found at ${{ env.LINT_SCRIPT_PATH }}"
            exit 1
          fi

      - name: Make lint script executable
        run: chmod +x ${{ env.LINT_SCRIPT_PATH }}

      - name: Run SQLFluff Lint Check Script
        id: lint_script
        run: bash ${{ env.LINT_SCRIPT_PATH }} --ci

      # --- Failure Handling ---
      # Intermediate step to read file content reliably on failure
      - name: Read Lint Output File on Failure
        id: read-lint-output
        # Run only if the lint_script step failed
        if: failure() && steps.lint_script.outcome == 'failure'
        run: |
          output_file="SQLFLUFF_LINTER_OUTPUT.TXT" # Matches script variable
          if [ -f "$output_file" ]; then
            # Read the file content, preserving newlines
            content=$(cat "$output_file")
          else
            content="Lint output file ($output_file) not found."
          fi
          content="${content//'%'/'%25'}"
          EOF_MARKER=$(uuidgen)
          echo "content<<${EOF_MARKER}" >> $GITHUB_OUTPUT
          echo "$content" >> $GITHUB_OUTPUT
          echo "${EOF_MARKER}" >> $GITHUB_OUTPUT
        shell: bash

      - name: Create Comment on PR Failure
        # Run only if lint_script failed and it's a pull request event
        if: failure() && steps.lint_script.outcome == 'failure' && github.event_name == 'pull_request'
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          token: ${{ secrets.GITHUB_TOKEN }}
          body: |
            ‚ùå **SQLFluff Linting Failed**

            Issues were found that require correction.
            Please review the output below (or the full logs), fix the issues locally, and commit the changes.
            
            Please run linter and follow instructions here: https://github.com/tuva-health/tuva/blob/main/CONTRIBUTING.md#how-to-run-the-linter

            ```text
            ${{ steps.read-lint-output.outputs.content }}
            ```

#######  DuckDB/MotherDuck
  duckdb_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and DuckDB adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-duckdb

      - name: Create dbt profile for DuckDB/MotherDuck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.DBT_MOTHERDUCK_TOKEN }}
        run: |
          mkdir -p ./profiles/duckdb
          python3 << 'EOF'
          import os
          import yaml
          
          # Create unique database name for CI run
          db_name = "my_db"
          
          # Build the MotherDuck connection string
          motherduck_path = f"md:{db_name}?motherduck_token={os.environ['MOTHERDUCK_TOKEN']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'duckdb',
                          'path': motherduck_path,
                          'threads': 4,
                          'settings': {
                              'custom_user_agent': 'dbt-ci'
                          }
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/duckdb/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          
          print(f"Created DuckDB/MotherDuck profile with database: {db_name}")
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/duckdb --vars '{"use_synthetic_data": true,"clinical_enabled": true,"claims_enabled": true,"provider_attribution_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  duckdb_claims_enabled:
    runs-on: ubuntu-latest
    needs: duckdb_clinical_and_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and DuckDB adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-duckdb

      - name: Create dbt profile for DuckDB/MotherDuck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.DBT_MOTHERDUCK_TOKEN }}
        run: |
          mkdir -p ./profiles/duckdb
          python3 << 'EOF'
          import os
          import yaml
          
          db_name = "my_db"
          motherduck_path = f"md:{db_name}?motherduck_token={os.environ['MOTHERDUCK_TOKEN']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'duckdb',
                          'path': motherduck_path,
                          'threads': 8,
                          'settings': {
                              'custom_user_agent': 'dbt-ci'
                          }
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/duckdb/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/duckdb --vars '{"use_synthetic_data": true,"claims_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  duckdb_claims_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: duckdb_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and DuckDB adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-duckdb

      - name: Create dbt profile for DuckDB/MotherDuck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.DBT_MOTHERDUCK_TOKEN }}
        run: |
          mkdir -p ./profiles/duckdb
          python3 << 'EOF'
          import os
          import yaml
          
          db_name = "my_db"
          motherduck_path = f"md:{db_name}?motherduck_token={os.environ['MOTHERDUCK_TOKEN']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'duckdb',
                          'path': motherduck_path,
                          'threads': 4,
                          'settings': {
                              'custom_user_agent': 'dbt-ci'
                          }
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/duckdb/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/duckdb --vars '{"use_synthetic_data": true,"claims_enabled":true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  duckdb_clinical_enabled:
    runs-on: ubuntu-latest
    needs: duckdb_claims_prov_attribution_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and DuckDB adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-duckdb

      - name: Create dbt profile for DuckDB/MotherDuck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.DBT_MOTHERDUCK_TOKEN }}
        run: |
          mkdir -p ./profiles/duckdb
          python3 << 'EOF'
          import os
          import yaml
          
          db_name = "my_db"
          motherduck_path = f"md:{db_name}?motherduck_token={os.environ['MOTHERDUCK_TOKEN']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'duckdb',
                          'path': motherduck_path,
                          'threads': 4,
                          'settings': {
                              'custom_user_agent': 'dbt-ci'
                          }
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/duckdb/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/duckdb --vars '{"use_synthetic_data": true,"clinical_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  duckdb_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: duckdb_clinical_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and DuckDB adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-duckdb

      - name: Create dbt profile for DuckDB/MotherDuck
        env:
          MOTHERDUCK_TOKEN: ${{ secrets.DBT_MOTHERDUCK_TOKEN }}
        run: |
          mkdir -p ./profiles/duckdb
          python3 << 'EOF'
          import os
          import yaml
          
          db_name = "my_db"
          motherduck_path = f"md:{db_name}?motherduck_token={os.environ['MOTHERDUCK_TOKEN']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'duckdb',
                          'path': motherduck_path,
                          'threads': 4,
                          'settings': {
                              'custom_user_agent': 'dbt-ci'
                          }
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/duckdb/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/duckdb --vars '{"use_synthetic_data": true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

#######  BigQuery
  bigquery_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and BigQuery adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-bigquery        

      - name: Create dbt profiles.yml for BigQuery
        run: |
          mkdir -p ./profiles/bigquery
          echo "default:
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: dev-ci-testing
                keyfile: ./creds.json
                dataset: connector
                threads: 8
                timeout_seconds: 300
                priority: interactive
            target: dev" > ./profiles/bigquery/profiles.yml
        working-directory: ci_testing

      - name: Create BigQuery credentials file
        run: |
          echo "${{ secrets.TUVA_BIGQUERY_TOKEN }}" | base64 --decode > ./creds.json
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/bigquery --vars '{"use_synthetic_data": true,"clinical_enabled": true,"claims_enabled": true,"fhir_preprocessing_enabled": true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  bigquery_claims_enabled:
    runs-on: ubuntu-latest
    needs: bigquery_clinical_and_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and BigQuery adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-bigquery        

      - name: Create dbt profiles.yml for BigQuery
        run: |
          mkdir -p ./profiles/bigquery
          echo "default:
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: dev-ci-testing
                keyfile: ./creds.json
                dataset: connector
                threads: 8
                timeout_seconds: 300
                priority: interactive
            target: dev" > ./profiles/bigquery/profiles.yml
        working-directory: ci_testing

      - name: Create BigQuery credentials file
        run: |
          echo "${{ secrets.TUVA_BIGQUERY_TOKEN }}" | base64 --decode > ./creds.json
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/bigquery --vars '{"use_synthetic_data": true,"claims_enabled": true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  bigquery_claims_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: bigquery_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and BigQuery adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-bigquery        

      - name: Create dbt profiles.yml for BigQuery
        run: |
          mkdir -p ./profiles/bigquery
          echo "default:
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: dev-ci-testing
                keyfile: ./creds.json
                dataset: connector
                threads: 8
                timeout_seconds: 300
                priority: interactive
            target: dev" > ./profiles/bigquery/profiles.yml
        working-directory: ci_testing

      - name: Create BigQuery credentials file
        run: |
          echo "${{ secrets.TUVA_BIGQUERY_TOKEN }}" | base64 --decode > ./creds.json
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/bigquery --vars '{"use_synthetic_data": true,"claims_enabled": true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  bigquery_clinical_enabled:
    runs-on: ubuntu-latest
    needs: bigquery_claims_prov_attribution_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and BigQuery adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-bigquery        

      - name: Create dbt profiles.yml for BigQuery
        run: |
          mkdir -p ./profiles/bigquery
          echo "default:
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: dev-ci-testing
                keyfile: ./creds.json
                dataset: connector
                threads: 8
                timeout_seconds: 300
                priority: interactive
            target: dev" > ./profiles/bigquery/profiles.yml
        working-directory: ci_testing

      - name: Create BigQuery credentials file
        run: |
          echo "${{ secrets.TUVA_BIGQUERY_TOKEN }}" | base64 --decode > ./creds.json
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/bigquery --vars '{"use_synthetic_data": true,"clinical_enabled": true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  bigquery_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: bigquery_clinical_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and BigQuery adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-bigquery        

      - name: Create dbt profiles.yml for BigQuery
        run: |
          mkdir -p ./profiles/bigquery
          echo "default:
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: dev-ci-testing
                keyfile: ./creds.json
                dataset: connector
                threads: 8
                timeout_seconds: 300
                priority: interactive
            target: dev" > ./profiles/bigquery/profiles.yml
        working-directory: ci_testing

      - name: Create BigQuery credentials file
        run: |
          echo "${{ secrets.TUVA_BIGQUERY_TOKEN }}" | base64 --decode > ./creds.json
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/bigquery --vars '{"use_synthetic_data": true,"provider_attribution": true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

#######  Databricks
  databricks_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      # Certifi addresses a known certificate issue with databricks and dbt
      - name: Install dbt-core and Databricks adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-databricks
          # Install SSL certificates fix
          pip install certifi

      - name: Set SSL certificate environment variables
        run: |
          echo "SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV
          echo "REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV

      - name: Create dbt profile for Databricks
        env:
          DATABRICKS_HOST: ${{ secrets.DBT_DATABRICKS_HOST }}
          DATABRICKS_HTTP_PATH: ${{ secrets.DBT_DATABRICKS_HTTP_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DBT_DATABRICKS_TOKEN }}
          DATABRICKS_CATALOG: ${{ secrets.DBT_DATABRICKS_CATALOG }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/databricks
          python3 << 'EOF'
          import os
          import yaml
          
          # Use schema suffix for CI isolation
          schema_name = os.environ.get('DATABRICKS_SCHEMA', 'ci_testing')
          if os.environ.get('SCHEMA_SUFFIX'):
              schema_name = f"ci_test_{os.environ['SCHEMA_SUFFIX']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'databricks',
                          'host': os.environ['DATABRICKS_HOST'],
                          'http_path': os.environ['DATABRICKS_HTTP_PATH'],
                          'token': os.environ['DATABRICKS_TOKEN'],
                          'catalog': os.environ.get('DATABRICKS_CATALOG', 'hive_metastore'),
                          'schema': schema_name,
                          'threads': 8,
                          'connect_timeout': 60,
                          'connect_retries': 3
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/databricks/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          
          print(f"Created Databricks profile with catalog={profile['default']['outputs']['dev']['catalog']}, schema={schema_name}")
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/databricks --vars '{"use_synthetic_data": true,"clinical_enabled": true,"claims_enabled": true,"provider_attribution_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  databricks_claims_enabled:
    runs-on: ubuntu-latest
    needs: databricks_clinical_and_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Databricks adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-databricks
          pip install certifi

      - name: Set SSL certificate environment variables
        run: |
          echo "SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV
          echo "REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV

      - name: Create dbt profile for Databricks
        env:
          DATABRICKS_HOST: ${{ secrets.DBT_DATABRICKS_HOST }}
          DATABRICKS_HTTP_PATH: ${{ secrets.DBT_DATABRICKS_HTTP_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DBT_DATABRICKS_TOKEN }}
          DATABRICKS_CATALOG: ${{ secrets.DBT_DATABRICKS_CATALOG }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/databricks
          python3 << 'EOF'
          import os
          import yaml
          
          schema_name = os.environ.get('DATABRICKS_SCHEMA', 'ci_testing')
          if os.environ.get('SCHEMA_SUFFIX'):
              schema_name = f"ci_test_{os.environ['SCHEMA_SUFFIX']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'databricks',
                          'host': os.environ['DATABRICKS_HOST'],
                          'http_path': os.environ['DATABRICKS_HTTP_PATH'],
                          'token': os.environ['DATABRICKS_TOKEN'],
                          'catalog': os.environ.get('DATABRICKS_CATALOG', 'hive_metastore'),
                          'schema': schema_name,
                          'threads': 8,
                          'connect_timeout': 60,
                          'connect_retries': 3
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/databricks/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/databricks --vars '{"use_synthetic_data": true,"claims_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  databricks_claims_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: databricks_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Databricks adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-databricks
          pip install certifi

      - name: Set SSL certificate environment variables
        run: |
          echo "SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV
          echo "REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV

      - name: Create dbt profile for Databricks
        env:
          DATABRICKS_HOST: ${{ secrets.DBT_DATABRICKS_HOST }}
          DATABRICKS_HTTP_PATH: ${{ secrets.DBT_DATABRICKS_HTTP_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DBT_DATABRICKS_TOKEN }}
          DATABRICKS_CATALOG: ${{ secrets.DBT_DATABRICKS_CATALOG }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/databricks
          python3 << 'EOF'
          import os
          import yaml
          
          schema_name = os.environ.get('DATABRICKS_SCHEMA', 'ci_testing')
          if os.environ.get('SCHEMA_SUFFIX'):
              schema_name = f"ci_test_{os.environ['SCHEMA_SUFFIX']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'databricks',
                          'host': os.environ['DATABRICKS_HOST'],
                          'http_path': os.environ['DATABRICKS_HTTP_PATH'],
                          'token': os.environ['DATABRICKS_TOKEN'],
                          'catalog': os.environ.get('DATABRICKS_CATALOG', 'hive_metastore'),
                          'schema': schema_name,
                          'threads': 8,
                          'connect_timeout': 60,
                          'connect_retries': 3
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/databricks/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/databricks --vars '{"use_synthetic_data": true,"claims_enabled":true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  databricks_clinical_enabled:
    runs-on: ubuntu-latest
    needs: databricks_claims_prov_attribution_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Databricks adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-databricks
          pip install certifi

      - name: Set SSL certificate environment variables
        run: |
          echo "SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV
          echo "REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV

      - name: Create dbt profile for Databricks
        env:
          DATABRICKS_HOST: ${{ secrets.DBT_DATABRICKS_HOST }}
          DATABRICKS_HTTP_PATH: ${{ secrets.DBT_DATABRICKS_HTTP_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DBT_DATABRICKS_TOKEN }}
          DATABRICKS_CATALOG: ${{ secrets.DBT_DATABRICKS_CATALOG }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/databricks
          python3 << 'EOF'
          import os
          import yaml
          
          schema_name = os.environ.get('DATABRICKS_SCHEMA', 'ci_testing')
          if os.environ.get('SCHEMA_SUFFIX'):
              schema_name = f"ci_test_{os.environ['SCHEMA_SUFFIX']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'databricks',
                          'host': os.environ['DATABRICKS_HOST'],
                          'http_path': os.environ['DATABRICKS_HTTP_PATH'],
                          'token': os.environ['DATABRICKS_TOKEN'],
                          'catalog': os.environ.get('DATABRICKS_CATALOG', 'hive_metastore'),
                          'schema': schema_name,
                          'threads': 8,
                          'connect_timeout': 60,
                          'connect_retries': 3
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/databricks/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/databricks --vars '{"use_synthetic_data": true,"clinical_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  databricks_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: databricks_clinical_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Databricks adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-databricks
          pip install certifi

      - name: Set SSL certificate environment variables
        run: |
          echo "SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV
          echo "REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt" >> $GITHUB_ENV

      - name: Create dbt profile for Databricks
        env:
          DATABRICKS_HOST: ${{ secrets.DBT_DATABRICKS_HOST }}
          DATABRICKS_HTTP_PATH: ${{ secrets.DBT_DATABRICKS_HTTP_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DBT_DATABRICKS_TOKEN }}
          DATABRICKS_CATALOG: ${{ secrets.DBT_DATABRICKS_CATALOG }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/databricks
          python3 << 'EOF'
          import os
          import yaml
          
          schema_name = os.environ.get('DATABRICKS_SCHEMA', 'ci_testing')
          if os.environ.get('SCHEMA_SUFFIX'):
              schema_name = f"ci_test_{os.environ['SCHEMA_SUFFIX']}"
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'databricks',
                          'host': os.environ['DATABRICKS_HOST'],
                          'http_path': os.environ['DATABRICKS_HTTP_PATH'],
                          'token': os.environ['DATABRICKS_TOKEN'],
                          'catalog': os.environ.get('DATABRICKS_CATALOG', 'hive_metastore'),
                          'schema': schema_name,
                          'threads': 8,
                          'connect_timeout': 60,
                          'connect_retries': 3
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/databricks/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/databricks
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/databricks --vars '{"use_synthetic_data": true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

#######  Fabric
  fabric_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ODBC Driver 18 for SQL Server
        run: |
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18          

      - name: Install dbt-core and Fabric adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-fabric          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/fabric --vars '{"use_synthetic_data": true,"clinical_enabled":true,"claims_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  fabric_claims_enabled:
    runs-on: ubuntu-latest
    needs: fabric_clinical_and_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ODBC Driver 18 for SQL Server
        run: |
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18          

      - name: Install dbt-core and Fabric adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-fabric          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/fabric --vars '{"use_synthetic_data": true,"claims_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  fabric_claims_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: fabric_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ODBC Driver 18 for SQL Server
        run: |
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18          

      - name: Install dbt-core and Fabric adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-fabric          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/fabric --vars '{"use_synthetic_data": true,"claims_enabled":true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  fabric_clinical_enabled:
    runs-on: ubuntu-latest
    needs: fabric_claims_prov_attribution_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ODBC Driver 18 for SQL Server
        run: |
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18          

      - name: Install dbt-core and Fabric adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-fabric          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/fabric --vars '{"use_synthetic_data": true,"clinical_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  fabric_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: fabric_clinical_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ODBC Driver 18 for SQL Server
        run: |
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18          

      - name: Install dbt-core and Fabric adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-fabric          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/fabric --vars '{"use_synthetic_data": true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

#######  Redshift
  redshift_clinical_and_claims_enabled:
    runs-on: ubuntu-latest
    needs: sqlfluff-lint

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt and Redshift adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-redshift==1.9.4

      - name: Create dbt profile
        env:
          REDSHIFT_HOST: ${{ secrets.REDSHIFT_SERVERLESS_TEST_HOST }}
          REDSHIFT_USER: ${{ secrets.REDSHIFT_SERVERLESS_TEST_USERNAME }}
          REDSHIFT_PASSWORD: ${{ secrets.REDSHIFT_SERVERLESS_TEST_PASSWORD }}
          REDSHIFT_DATABASE: ${{ secrets.REDSHIFT_SERVERLESS_TEST_DATABASE }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/redshift
          python3 << 'EOF'
          import os
          import yaml
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'redshift',
                          'host': os.environ['REDSHIFT_HOST'],
                          'port': 5439,
                          'user': os.environ['REDSHIFT_USER'],
                          'password': os.environ['REDSHIFT_PASSWORD'],
                          'dbname': os.environ['REDSHIFT_DATABASE'],
                          'schema': f"ci_test_{os.environ['SCHEMA_SUFFIX']}",
                          'threads': 32,
                          'sslmode': 'require',
                          'is_serverless': True,
                          'connect_timeout': 120
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/redshift/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt deps
        run: dbt deps --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt debug
        run: dbt debug --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/redshift --vars '{"use_synthetic_data": true,"clinical_enabled":true,"claims_enabled":true,"provider_attribution_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  redshift_claims_enabled:
    runs-on: ubuntu-latest
    needs: redshift_clinical_and_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt and Redshift adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-redshift==1.9.4

      - name: Create dbt profile
        env:
          REDSHIFT_HOST: ${{ secrets.REDSHIFT_SERVERLESS_TEST_HOST }}
          REDSHIFT_USER: ${{ secrets.REDSHIFT_SERVERLESS_TEST_USERNAME }}
          REDSHIFT_PASSWORD: ${{ secrets.REDSHIFT_SERVERLESS_TEST_PASSWORD }}
          REDSHIFT_DATABASE: ${{ secrets.REDSHIFT_SERVERLESS_TEST_DATABASE }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/redshift
          python3 << 'EOF'
          import os
          import yaml
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'redshift',
                          'host': os.environ['REDSHIFT_HOST'],
                          'port': 5439,
                          'user': os.environ['REDSHIFT_USER'],
                          'password': os.environ['REDSHIFT_PASSWORD'],
                          'dbname': os.environ['REDSHIFT_DATABASE'],
                          'schema': f"ci_test_{os.environ['SCHEMA_SUFFIX']}",
                          'threads': 32,
                          'sslmode': 'require',
                          'is_serverless': True,
                          'connect_timeout': 120
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/redshift/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt deps
        run: dbt deps --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt debug
        run: dbt debug --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/redshift --vars '{"use_synthetic_data": true,"claims_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  redshift_claims_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: redshift_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt and Redshift adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-redshift==1.9.4

      - name: Create dbt profile
        env:
          REDSHIFT_HOST: ${{ secrets.REDSHIFT_SERVERLESS_TEST_HOST }}
          REDSHIFT_USER: ${{ secrets.REDSHIFT_SERVERLESS_TEST_USERNAME }}
          REDSHIFT_PASSWORD: ${{ secrets.REDSHIFT_SERVERLESS_TEST_PASSWORD }}
          REDSHIFT_DATABASE: ${{ secrets.REDSHIFT_SERVERLESS_TEST_DATABASE }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/redshift
          python3 << 'EOF'
          import os
          import yaml
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'redshift',
                          'host': os.environ['REDSHIFT_HOST'],
                          'port': 5439,
                          'user': os.environ['REDSHIFT_USER'],
                          'password': os.environ['REDSHIFT_PASSWORD'],
                          'dbname': os.environ['REDSHIFT_DATABASE'],
                          'schema': f"ci_test_{os.environ['SCHEMA_SUFFIX']}",
                          'threads': 32,
                          'sslmode': 'require',
                          'is_serverless': True,
                          'connect_timeout': 120
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/redshift/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt deps
        run: dbt deps --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt debug
        run: dbt debug --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/redshift --vars '{"use_synthetic_data": true,"claims_enabled":true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  redshift_clinical_enabled:
    runs-on: ubuntu-latest
    needs: redshift_claims_prov_attribution_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt and Redshift adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-redshift==1.9.4

      - name: Create dbt profile
        env:
          REDSHIFT_HOST: ${{ secrets.REDSHIFT_SERVERLESS_TEST_HOST }}
          REDSHIFT_USER: ${{ secrets.REDSHIFT_SERVERLESS_TEST_USERNAME }}
          REDSHIFT_PASSWORD: ${{ secrets.REDSHIFT_SERVERLESS_TEST_PASSWORD }}
          REDSHIFT_DATABASE: ${{ secrets.REDSHIFT_SERVERLESS_TEST_DATABASE }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/redshift
          python3 << 'EOF'
          import os
          import yaml
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'redshift',
                          'host': os.environ['REDSHIFT_HOST'],
                          'port': 5439,
                          'user': os.environ['REDSHIFT_USER'],
                          'password': os.environ['REDSHIFT_PASSWORD'],
                          'dbname': os.environ['REDSHIFT_DATABASE'],
                          'schema': f"ci_test_{os.environ['SCHEMA_SUFFIX']}",
                          'threads': 32,
                          'sslmode': 'require',
                          'is_serverless': True,
                          'connect_timeout': 120
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/redshift/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt deps
        run: dbt deps --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt debug
        run: dbt debug --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/redshift --vars '{"use_synthetic_data": true,"clinical_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  redshift_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: redshift_clinical_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt and Redshift adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-redshift==1.9.4

      - name: Create dbt profile
        env:
          REDSHIFT_HOST: ${{ secrets.REDSHIFT_SERVERLESS_TEST_HOST }}
          REDSHIFT_USER: ${{ secrets.REDSHIFT_SERVERLESS_TEST_USERNAME }}
          REDSHIFT_PASSWORD: ${{ secrets.REDSHIFT_SERVERLESS_TEST_PASSWORD }}
          REDSHIFT_DATABASE: ${{ secrets.REDSHIFT_SERVERLESS_TEST_DATABASE }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/redshift
          python3 << 'EOF'
          import os
          import yaml
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'redshift',
                          'host': os.environ['REDSHIFT_HOST'],
                          'port': 5439,
                          'user': os.environ['REDSHIFT_USER'],
                          'password': os.environ['REDSHIFT_PASSWORD'],
                          'dbname': os.environ['REDSHIFT_DATABASE'],
                          'schema': f"ci_test_{os.environ['SCHEMA_SUFFIX']}",
                          'threads': 32,
                          'sslmode': 'require',
                          'is_serverless': True,
                          'connect_timeout': 120
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/redshift/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt deps
        run: dbt deps --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt debug
        run: dbt debug --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/redshift --vars '{"use_synthetic_data": true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

#######  Snowflake
  snowflake_clinical_and_claims_enabled:
    runs-on: ubuntu-latest
    needs: sqlfluff-lint

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake        

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"clinical_enabled":true,"claims_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_claims_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_clinical_and_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"claims_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_claims_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"claims_enabled":true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_clinical_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_claims_prov_attribution_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"clinical_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_clinical_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake          

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_semantic_layer_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_claims_enabled

    steps:
      - uses: actions/checkout@v4
        with:
          ref: refs/pull/${{ github.event.inputs.prNumber }}/merge

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"claims_enabled":true,"semantic_layer_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

#######  Post message to the PR with the status of each job (i.e. success or failure)
  post_status_to_PR:
    needs: [
      sqlfluff-lint,
      duckdb_clinical_and_claims_enabled,
      duckdb_claims_enabled,
      duckdb_claims_prov_attribution_enabled,
      duckdb_clinical_enabled,
      duckdb_prov_attribution_enabled,
      bigquery_clinical_and_claims_enabled,
      bigquery_claims_enabled,
      bigquery_claims_prov_attribution_enabled,
      bigquery_clinical_enabled,
      bigquery_prov_attribution_enabled,
      databricks_clinical_and_claims_enabled,
      databricks_claims_enabled,
      databricks_claims_prov_attribution_enabled,
      databricks_clinical_enabled,
      databricks_prov_attribution_enabled,
      fabric_clinical_and_claims_enabled,
      fabric_claims_enabled,
      fabric_claims_prov_attribution_enabled,
      fabric_clinical_enabled,
      fabric_prov_attribution_enabled,
      redshift_clinical_and_claims_enabled,
      redshift_claims_enabled,
      redshift_claims_prov_attribution_enabled,
      redshift_clinical_enabled,
      redshift_prov_attribution_enabled,
      snowflake_clinical_and_claims_enabled,
      snowflake_claims_enabled,
      snowflake_claims_prov_attribution_enabled,
      snowflake_clinical_enabled,
      snowflake_prov_attribution_enabled,
      snowflake_semantic_layer_enabled
    ]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Post comment on PR
        run: |
          PR_COMMENT="Workflow has finished with the following statuses:
          <ul><li>Linter: ${{ needs.sqlfluff-lint.result }}</li></ul>
          <ul><li>DuckDB Clinical and Claims: ${{ needs.duckdb_clinical_and_claims_enabled.result }}</li></ul>
          <ul><li>DuckDB Claims: ${{ needs.duckdb_claims_enabled.result }}</li></ul>
          <ul><li>DuckDB Clinical: ${{ needs.duckdb_clinical_enabled.result }}</li></ul>
          <ul><li>BigQuery Clinical and Claims: ${{ needs.bigquery_clinical_and_claims_enabled.result }}</li></ul>
          <ul><li>BigQuery Claims: ${{ needs.bigquery_claims_enabled.result }}</li></ul>
          <ul><li>BigQuery Clinical: ${{ needs.bigquery_clinical_enabled.result }}</li></ul>
          <ul><li>Databricks Clinical and Claims: ${{ needs.databricks_clinical_and_claims_enabled.result }}</li></ul>
          <ul><li>Databricks Claims: ${{ needs.databricks_claims_enabled.result }}</li></ul>
          <ul><li>Databricks Clinical: ${{ needs.databricks_clinical_enabled.result }}</li></ul>
          <ul><li>Fabric Clinical and Claims: ${{ needs.fabric_clinical_and_claims_enabled.result }}</li></ul>
          <ul><li>Fabric Claims: ${{ needs.fabric_claims_enabled.result }}</li></ul>
          <ul><li>Fabric Clinical: ${{ needs.fabric_clinical_enabled.result }}</li></ul>
          <ul><li>Redshift Clinical and Claims: ${{ needs.redshift_clinical_and_claims_enabled.result }}</li></ul>
          <ul><li>Redshift Claims: ${{ needs.redshift_claims_enabled.result }}</li></ul>
          <ul><li>Redshift Clinical: ${{ needs.redshift_clinical_enabled.result }}</li></ul>
          <ul><li>Snowflake Clinical and Claims: ${{ needs.snowflake_clinical_and_claims_enabled.result }}</li></ul>
          <ul><li>Snowflake Claims: ${{ needs.snowflake_claims_enabled.result }}</li></ul>
          <ul><li>Snowflake Clinical: ${{ needs.snowflake_clinical_enabled.result }}</li></ul>
          <ul><li>Snowflake Semantic Layer: ${{ needs.snowflake_semantic_layer_enabled.result }}</li></ul>"
          PR_ID=$(gh pr view https://github.com/${{ github.repository }}/pull/${{ github.event.inputs.prNumber }} --json number -q .number)
          gh pr comment $PR_ID --body "$PR_COMMENT"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
