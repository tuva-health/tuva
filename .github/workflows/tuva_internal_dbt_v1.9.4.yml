name: tuva_internal_dbt_v1.9.4
on:
  pull_request:
    branches:
      - main
      - 'minor-release*'
      - 'release*'
  workflow_dispatch:  # for manual triggering

concurrency:
  # Ensure only one instance of this workflow runs across all PRs to prevent database conflicts
  group: ci-testing
  # Don't cancel in-progress runs automatically
  cancel-in-progress: false

env:
  PYTHON_VERSION: '3.11'
  LINT_SCRIPT_PATH: ./lint_tuva_project.sh
  DUCKDB_VERSION: '0.9.2'

#######  Secrets #######
#######  BigQuery
  TUVA_BIGQUERY_TOKEN: ${{ secrets.TUVA_BIGQUERY_TOKEN }}
  TUVA_BIGQUERY_PROJECT: ${{ secrets.TUVA_BIGQUERY_PROJECT }}
#######  Fabric
  DBT_FABRIC_CI_HOST: ${{ secrets.DBT_FABRIC_CI_HOST }}
  DBT_FABRIC_SERVICE_PRINCIPAL_ID: ${{ secrets.DBT_FABRIC_SERVICE_PRINCIPAL_ID }}
  DBT_FABRIC_SERVICE_PRINCIPAL_SECRET: ${{ secrets.DBT_FABRIC_SERVICE_PRINCIPAL_SECRET }}
  DBT_FABRIC_TENANT_ID: ${{ secrets.DBT_FABRIC_TENANT_ID }}
  DBT_FABRIC_CI_DATABASE: ${{ secrets.DBT_FABRIC_CI_DATABASE }}
  DBT_FABRIC_CI_SCHEMA: ${{ secrets.DBT_FABRIC_CI_SCHEMA }}
#######  Snowflake
  DBT_TUVA_SNOWFLAKE_ACCOUNT: ${{ secrets.DBT_TUVA_SNOWFLAKE_ACCOUNT }}
  DBT_TUVA_CI_DATABASE: ${{ secrets.DBT_TUVA_CI_DATABASE }}
  DBT_SNOWFLAKE_CI_PASSWORD: ${{ secrets.DBT_SNOWFLAKE_CI_PASSWORD }}
  DBT_SNOWFLAKE_CI_ROLE: ${{ secrets.DBT_SNOWFLAKE_CI_ROLE }}
  DBT_SNOWFLAKE_CI_SCHEMA: ${{ secrets.DBT_SNOWFLAKE_CI_SCHEMA }}
  DBT_SNOWFLAKE_CI_USER: ${{ secrets.DBT_SNOWFLAKE_CI_USER }}
  DBT_SNOWFLAKE_CI_WAREHOUSE: ${{ secrets.DBT_SNOWFLAKE_CI_WAREHOUSE }}

# Permissions needed for the comment action
permissions:
  contents: read
  pull-requests: write

jobs:
  # ============================================================================
  # Linting Job (Runs First)
  # ============================================================================
  sqlfluff-lint:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          # Install dbt adapter needed for compile/lint (using Snowflake here)
          # Also install sqlfluff
          pip install dbt-core==1.9.4 dbt-snowflake sqlfluff==3.3.1 sqlfluff-templater-dbt

      # --- Setup dbt Profile for Linting ---
      - name: Create dbt profiles directory
        run: mkdir -p ~/.dbt

      - name: Create dbt profiles.yml for Snowflake (for linting context)
        run: |
          echo "default:
            outputs:
              dev:
                account: \"{{ env_var('DBT_TUVA_SNOWFLAKE_ACCOUNT') }}\"
                database: dev_ci_testing
                password: \"{{ env_var('DBT_SNOWFLAKE_CI_PASSWORD') }}\"
                role: \"{{ env_var('DBT_SNOWFLAKE_CI_ROLE') }}\"
                schema: \"{{ env_var('DBT_SNOWFLAKE_CI_SCHEMA') }}\"
                threads: 8
                type: snowflake
                user: \"{{ env_var('DBT_SNOWFLAKE_CI_USER') }}\"
                warehouse: \"{{ env_var('DBT_SNOWFLAKE_CI_WAREHOUSE') }}\"
            target: dev" > ~/.dbt/profiles.yml

      - name: Check if lint script exists
        run: |
          if [ ! -f "${{ env.LINT_SCRIPT_PATH }}" ]; then
            echo "Error: Lint script not found at ${{ env.LINT_SCRIPT_PATH }}"
            exit 1
          fi

      - name: Make lint script executable
        run: chmod +x ${{ env.LINT_SCRIPT_PATH }}

      - name: Run SQLFluff Lint Check Script
        id: lint_script
        run: bash ${{ env.LINT_SCRIPT_PATH }} --ci

      # --- Failure Handling ---
      # Intermediate step to read file content reliably on failure
      - name: Read Lint Output File on Failure
        id: read-lint-output
        # Run only if the lint_script step failed
        if: failure() && steps.lint_script.outcome == 'failure'
        run: |
          output_file="SQLFLUFF_LINTER_OUTPUT.TXT" # Matches script variable
          if [ -f "$output_file" ]; then
            # Read the file content, preserving newlines
            content=$(cat "$output_file")
          else
            content="Lint output file ($output_file) not found."
          fi
          content="${content//'%'/'%25'}"
          EOF_MARKER=$(uuidgen)
          echo "content<<${EOF_MARKER}" >> $GITHUB_OUTPUT
          echo "$content" >> $GITHUB_OUTPUT
          echo "${EOF_MARKER}" >> $GITHUB_OUTPUT
        shell: bash

      - name: Create Comment on PR Failure
        # Run only if lint_script failed and it's a pull request event
        if: failure() && steps.lint_script.outcome == 'failure' && github.event_name == 'pull_request'
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          token: ${{ secrets.GITHUB_TOKEN }}
          body: |
            âŒ **SQLFluff Linting Failed**

            Issues were found that require manual correction or are unfixable by `sqlfluff fix`.
            Please review the output below (or the full logs), fix the issues locally, and commit the changes.

            ```text
            ${{ steps.read-lint-output.outputs.content }}
            ```
  ####### DuckDB - Clinical and Claims Enabled
  duckdb_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and DuckDB adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-duckdb==1.9.0
          # Install DuckDB Python package for any direct DB operations if needed
          pip install duckdb==${{ env.DUCKDB_VERSION }}

      - name: Create dbt profiles directory for DuckDB
        run: mkdir -p ./profiles/duckdb
        working-directory: ci_testing

      - name: Create dbt profiles.yml for DuckDB
        run: |
          # Use a file-based database in the current directory
          # Each job gets its own database file to avoid conflicts
          echo "default:
            outputs:
              dev:
                type: duckdb
                path: ':memory:'
                threads: 8
                settings:
                  memory_limit: '12GB'
                  max_memory: '12GB'
                  temp_directory: './tmp'
            target: dev" > ./profiles/duckdb/profiles.yml
        working-directory: ci_testing

      - name: Create temp directory for DuckDB
        run: mkdir -p ./tmp
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/duckdb
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/duckdb \
            --vars '{"use_synthetic_data": true,"clinical_enabled": true,"claims_enabled": true,"provider_attribution_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Cleanup DuckDB files
        if: always()
        run: |
          rm -f ./tuva_ci_${{ github.run_id }}_*.duckdb
          rm -f ./tuva_ci_${{ github.run_id }}_*.duckdb.wal
          rm -rf ./tmp
        working-directory: ci_testing

  # ============================================================================
  # Build Jobs (Run only if Linting Passes)
  # ============================================================================

  #######  BigQuery
  bigquery_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and BigQuery adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-bigquery

      - name: Create dbt profiles.yml for BigQuery
        run: |
          mkdir -p ./profiles/bigquery
          echo "default:
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: dev-ci-testing
                keyfile: ./creds.json
                dataset: connector
                threads: 8
                timeout_seconds: 300
                priority: interactive
            target: dev" > ./profiles/bigquery/profiles.yml
        working-directory: ci_testing

      - name: Create BigQuery credentials file
        run: |
          echo "${{ secrets.TUVA_BIGQUERY_TOKEN }}" | base64 --decode > ./creds.json
        working-directory: ci_testing

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/bigquery
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/bigquery --vars '{"use_synthetic_data": true,"clinical_enabled": true,"claims_enabled": true,"provider_attribution_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash


#######  Fabric
  fabric_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install ODBC Driver 18 for SQL Server
        run: |
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
          curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18

      - name: Install dbt-core and Fabric adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-fabric

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/fabric
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/fabric --vars '{"use_synthetic_data": true,"clinical_enabled":true,"claims_enabled":true,"provider_attribution_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

#######  Redshift
  redshift_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt and Redshift adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-redshift==1.9.4

      - name: Create dbt profile
        env:
          REDSHIFT_HOST: ${{ secrets.REDSHIFT_SERVERLESS_TEST_HOST }}
          REDSHIFT_USER: ${{ secrets.REDSHIFT_SERVERLESS_TEST_USERNAME }}
          REDSHIFT_PASSWORD: ${{ secrets.REDSHIFT_SERVERLESS_TEST_PASSWORD }}
          REDSHIFT_DATABASE: ${{ secrets.REDSHIFT_SERVERLESS_TEST_DATABASE }}
          SCHEMA_SUFFIX: ${{ github.run_id }}
        run: |
          mkdir -p ./profiles/redshift
          python3 << 'EOF'
          import os
          import yaml
          
          profile = {
              'default': {
                  'outputs': {
                      'dev': {
                          'type': 'redshift',
                          'host': os.environ['REDSHIFT_HOST'],
                          'port': 5439,
                          'user': os.environ['REDSHIFT_USER'],
                          'password': os.environ['REDSHIFT_PASSWORD'],
                          'dbname': os.environ['REDSHIFT_DATABASE'],
                          'schema': f"ci_test_{os.environ['SCHEMA_SUFFIX']}",
                          'threads': 32,
                          'sslmode': 'require',
                          'is_serverless': True,
                          'connect_timeout': 120
                      }
                  },
                  'target': 'dev'
              }
          }
          
          with open('./profiles/redshift/profiles.yml', 'w') as f:
              yaml.dump(profile, f, default_flow_style=False)
          EOF
        working-directory: ci_testing

      - name: dbt deps
        run: dbt deps --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt debug
        run: dbt debug --profiles-dir ./profiles/redshift
        working-directory: ci_testing

      - name: dbt build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/redshift \
            --vars '{"use_synthetic_data": true,"clinical_enabled": true,"claims_enabled": true,"provider_attribution_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

#######  Snowflake
  snowflake_clinical_and_claims_enabled:
    needs: sqlfluff-lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-build
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"clinical_enabled":true,"claims_enabled":true,"fhir_preprocessing_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_claims_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_clinical_and_claims_enabled

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"claims_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_claims_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_claims_enabled

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"claims_enabled":true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_clinical_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_claims_prov_attribution_enabled

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt build --full-refresh --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"clinical_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_prov_attribution_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_clinical_enabled

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"provider_attribution_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash

  snowflake_semantic_layer_enabled:
    runs-on: ubuntu-latest
    needs: snowflake_claims_enabled

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dbt-core and Snowflake adapter
        run: |
          python -m pip install --upgrade pip
          pip install dbt-core==1.9.4 dbt-snowflake

      - name: dbt-deps
        run: dbt deps --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      - name: dbt-debug
        run: dbt debug --profiles-dir ./profiles/snowflake
        working-directory: ci_testing

      # Changed to compile to save processing time
      - name: dbt-compile
        run: |
          dbt compile --profiles-dir ./profiles/snowflake --vars '{"use_synthetic_data": true,"claims_enabled":true,"semantic_layer_enabled":true}'
        working-directory: ci_testing

      - name: Get the result
        if: ${{ always() }}
        run: echo "${{ steps.dbt-build.outputs.result }}"
        shell: bash